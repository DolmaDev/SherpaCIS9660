# -*- coding: utf-8 -*-
"""Proj1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DJko2hxip5ImuPBFdc9BbYG6psSBbVyi
"""

!pip install prophet

"""1. Data Loading and Initial Exploration"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import json
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import GridSearchCV
from prophet import Prophet
from pathlib import Path
import xgboost as xgb
import warnings
warnings.filterwarnings('ignore')


# Set display options
pd.set_option('display.max_columns', None)
plt.style.use('default')

print('Libraries imported successfully')

"""Load the dataset and explore"""

# File path (local path or relative to notebook)
file_path = "Online Retail.xlsx"

# Basic info
print(f"Dataset shape: {df.shape}")
print("\nPreview:")
display(df.head())

"""Basic information about the dataset"""

print('Dataset Info:')
df.info()

"""Generate descriptive stats"""

df.describe()

"""Check for missing values"""

print('Missing values per column:')
missing_values = df.isnull().sum()
print(missing_values)

# finding the total missing values
print(f'Total Missing Values: {missing_values.sum()}')
# find the percentage of missing data
print(f'Percentage of Missing Values: {(missing_values.sum() /len(df)) * 100:.2f}%')

#Work on a copy of original df
df_clean = df.copy()

# Drop columns we are NOT using
# Country removed to prevent high-cardinality dummy vars
df_clean = df_clean.drop(columns=["CustomerID", "Country"], errors="ignore")

# Ensure correct data types
df_clean['InvoiceDate'] = pd.to_datetime(df_clean['InvoiceDate'], errors="coerce")
df_clean["Quantity"]    = pd.to_numeric(df_clean["Quantity"], errors="coerce")
df_clean["UnitPrice"]   = pd.to_numeric(df_clean["UnitPrice"], errors="coerce")

# Remove missing essentials
df_clean = df_clean.dropna(subset=["InvoiceDate", "Quantity", "UnitPrice"])

# Remove returns / free items
before = len(df_clean)
df_clean = df_clean[(df_clean["Quantity"] > 0) & (df_clean["UnitPrice"] > 0)].copy() after = len(df_clean)
print(f"Kept {after:,} rows ({after/before:.1%}) after filtering returns/free items.")

#Create Revenue column
df_clean['Revenue'] = df_clean['Quantity'] * df_clean['UnitPrice']

#Time-based features (for trend analysis)
df_clean['YearMonth'] = df_clean['InvoiceDate'].dt.to_period('M')
df_clean['Week']      = df_clean['InvoiceDate'].dt.isocalendar().week
df_clean['Day']       = df_clean['InvoiceDate'].dt.date
df_clean['DayOfWeek'] = df_clean['InvoiceDate'].dt.day_name()

#Aggregate MONTHLY revenue time series
monthly_df = (df_clean.groupby(df_clean['InvoiceDate'].dt.to_period('M'))['Revenue'].sum().reset_index())

# Convert Period type to Timestamp for plotting / Prophet
monthly_df['InvoiceDate'] = monthly_df['InvoiceDate'].dt.to_timestamp()
monthly_df = monthly_df.set_index('InvoiceDate').sort_index()

# Quick check
print(monthly_df.head())
print(df_clean.head())

"""Exploratory Data Analysis and Visualization"""

fig, axes = plt.subplots(3, 2, figsize=(16, 18))
plt.subplots_adjust(hspace=0.4, wspace=0.3)
# 1) Revenue distribution (overall)
plt.subplot(3, 2, 1)
plt.hist(df_clean['Revenue'], bins=50, edgecolor='black', alpha=0.7)
plt.title('Distribution of Revenue')
plt.xlabel('Revenue (£)')
plt.ylabel('Frequency')

# 2) Boxplot by Month to show monthly revenue spread and outliers
plt.subplot(3, 2, 2)
sns.boxplot(x=df_clean['YearMonth'].astype(str), y='Revenue', data=df_clean)
plt.xticks(rotation=45)
plt.title('Monthly Revenue Distribution')
plt.xlabel('Year-Month')
plt.ylabel('Revenue (£)')

# 3) Monthly total revenue trend line
plt.subplot(3, 2, 3)
monthly_revenue = df_clean.groupby('YearMonth')['Revenue'].sum().reset_index()
monthly_revenue['YearMonth'] = monthly_revenue['YearMonth'].dt.to_timestamp()
plt.plot(monthly_revenue['YearMonth'], monthly_revenue['Revenue'], marker='o')
plt.title('Monthly Revenue Trend')
plt.xlabel('Month')
plt.ylabel('Total Revenue (£)')
plt.grid(True)

# 4) Weekly revenue trend with rolling average (smoothes noise)
plt.subplot(3, 2, 4)
weekly_revenue = df_clean.groupby('Week')['Revenue'].sum().reset_index()
weekly_revenue['RollingMean'] = weekly_revenue['Revenue'].rolling(window=4).mean()
plt.plot(weekly_revenue['Week'], weekly_revenue['Revenue'], label='Weekly Revenue', alpha=0.6)
plt.plot(weekly_revenue['Week'], weekly_revenue['RollingMean'], label='4-Week Rolling Avg', linewidth=2)
plt.title('Weekly Revenue Trend with Rolling Average')
plt.xlabel('Week Number')
plt.ylabel('Total Revenue (£)')
plt.legend()

# 5) Daily revenue trend
plt.subplot(3, 2, 5)
daily_revenue = df_clean.groupby('Day')['Revenue'].sum().reset_index()
plt.plot(daily_revenue['Day'], daily_revenue['Revenue'], alpha=0.7)
plt.title('Daily Revenue Over Time')
plt.xlabel('Date')
plt.ylabel('Revenue (£)')
plt.xticks(rotation=45)

# 6) Revenue by Day of Week (aggregated over entire dataset)
plt.subplot(3, 2, 6)
day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
dow_revenue = df_clean.groupby(df_clean['InvoiceDate'].dt.day_name())['Revenue'].sum().reindex(day_order)
sns.barplot(x=dow_revenue.index, y=dow_revenue.values, palette='Blues_d')
plt.title('Total Revenue by Day of Week')
plt.xlabel('Day of Week')
plt.ylabel('Total Revenue (£)')

plt.tight_layout()
plt.show()

"""Correlation Matrix"""

# Correlation matrix for sales features (Pearson & Spearman)

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Ensure datetime + revenue exist
df_clean["InvoiceDate"] = pd.to_datetime(df_clean["InvoiceDate"], errors="coerce")
df_clean["Revenue"] = df_clean["Quantity"] * df_clean["UnitPrice"]

# Minimal time features (numeric)
df_clean["Month"] = df_clean["InvoiceDate"].dt.month
df_clean["DayOfWeek"] = df_clean["InvoiceDate"].dt.dayofweek  # Mon=0..Sun=6

# Choose numeric features only
num_cols = ["Quantity", "UnitPrice", "Revenue", "Month", "DayOfWeek"]
num_df = df_clean[num_cols].dropna()

# Compute correlations
corr_pearson = num_df.corr(method="pearson")
plt.figure(figsize=(14, 6))

plt.subplot(1, 1, 1)
sns.heatmap(corr_pearson, annot=True, cmap="coolwarm", center=0, square=True, linewidths=0.5)
plt.title("Correlation Matrix (Pearson) — Sales Features")

plt.tight_layout()
plt.show()

# Select only numeric columns
num_df = df_clean.select_dtypes(include=[np.number])

# Pearson correlations with Revenue
print("Correlations with Revenue (Pearson, sorted by absolute value):")
revenue_correlations_pearson = (num_df.corr(method="pearson")["Revenue"].drop("Revenue").abs().sort_values(ascending=False))
print(revenue_correlations_pearson)

"""Scatter Plot of most correlated features"""

# Ensure Revenue exists
df_clean["Revenue"] = df_clean["Quantity"] * df_clean["UnitPrice"]

# Pearson correlations with Revenue
corr_p = (
    df_clean.select_dtypes(include=[np.number])
    .corr(method="pearson")["Revenue"]
    .drop("Revenue")
    .sort_values(key=abs, ascending=False)
)

# Plotting top 3 correlated features
top_feats = corr_p.head(3).index
plt.figure(figsize=(18, 4))
for i, feat in enumerate(top_feats, 1):
    plt.subplot(1, 3, i)
    plt.scatter(df_clean[feat], df_clean["Revenue"], alpha=0.4, s=8)
    z = np.polyfit(df_clean[feat], df_clean["Revenue"], 1)
    plt.plot(sorted(df_clean[feat]), np.poly1d(z)(sorted(df_clean[feat])), "r--")
    plt.xlabel(feat)
    plt.ylabel("Revenue (£)")
    plt.title(f"{feat} vs Revenue\nr = {corr_p[feat]:.3f}")
plt.suptitle("Top 3 Pearson Correlations with Revenue")
plt.tight_layout()
plt.show()

"""Data Processing

Feature Engineering and Aggregation
"""

#Start from cleaned transaction-level data
df_daily = df_clean.copy()
df_daily['InvoiceDate'] = pd.to_datetime(df_daily['InvoiceDate'], errors='coerce')

#Create revenue column
df_daily['Revenue'] = df_daily['Quantity'] * df_daily['UnitPrice']

#Aggregate daily features
daily_df = (
    df_daily.groupby(df_daily['InvoiceDate'].dt.date)
    .agg(
        total_revenue=('Revenue', 'sum'),           # Target: Total revenue (was 'y')
        total_quantity=('Quantity', 'sum'),         # Total quantity sold
        avg_unit_price=('UnitPrice', 'mean')        # Average price
    )
    .reset_index()
    .rename(columns={'InvoiceDate': 'ds'})
)


#Convert to datetime
daily_df['ds'] = pd.to_datetime(daily_df['ds'])
daily_df = daily_df.set_index('ds').asfreq('D').reset_index()

daily_df[['total_revenue', 'total_quantity']] = daily_df[['total_revenue', 'total_quantity']].fillna(0)
daily_df['avg_unit_price'] = daily_df['avg_unit_price'].fillna(0)

#Add time-based features
daily_df['day_of_week'] = daily_df['ds'].dt.dayofweek # Monday=0, Sunday=6
daily_df['month'] = daily_df['ds'].dt.month
daily_df['day_of_year'] = daily_df['ds'].dt.dayofyear
daily_df['is_Saturday'] = (daily_df['day_of_week'] == 5).astype(int)
daily_df['day_name'] = daily_df['ds'].dt.day_name()

#Created cyclical features for day_of_year to capture seasonality
daily_df['day_of_year_sin'] = np.sin(2 * np.pi * daily_df['day_of_year'] / 365)
daily_df['day_of_year_cos'] = np.cos(2 * np.pi * daily_df['day_of_year'] / 365)

#Lag features
# Sort by date first (important!)
daily_df = daily_df.sort_values('ds').reset_index(drop=True)

# Lag: previous day's revenue
daily_df['prev_day_rev'] = daily_df['total_revenue'].shift(1)

# Rolling mean: past 7 days revenue (excluding current day)
daily_df['avg_rev_last_7_days'] = (daily_df['total_revenue'].shift(1).rolling(window=7, min_periods=1).mean())

# 14-day lag feature
daily_df['prev_day_rev_14'] = daily_df['total_revenue'].shift(14)

# Remove rows with NaNs caused by shift
daily_df = daily_df.dropna().reset_index(drop=True)

print(daily_df.head(10))
print("\nShape:", daily_df.shape)

"""Train-Test Split"""

daily_df

feature_cols = ['day_of_year_sin', 'day_of_year_cos', 'day_of_week','month', 'prev_day_rev', 'avg_rev_last_7_days', 'prev_day_rev_14']

X = daily_df[feature_cols]
y = daily_df['total_revenue']

X

y = daily_df['total_revenue']

y

# features shape
print(f'Features shape: {X.shape}')
print(f'Target shape: {y.shape}')

# get feature names
print(f'Feature names: {list(X.columns)}')

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

# take a look at the shapes of the training and testing sets
print(f'\n Training set shape {X_train.shape}')
print(f'Test set shape {X_test.shape}')
print(f'Training target set shape {y_train.shape}')
print(f'Testing target set shape {y_test.shape}')

X_train

# Feature scaling using StandardScaler
print('Before scaling - Training set statistics')
print(X_train.describe())

# Initialize the scaler
scaler = StandardScaler()

# Fit the scaler on the training data and transform both training and testing data
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled  = scaler.transform(X_test)

type(X_train_scaled)

# Convert back to DataFrame for easier handling
X_train_scaled = pd.DataFrame(X_train_scaled, columns = X_train.columns, index = X_train.index)
X_test_scaled = pd.DataFrame(X_test_scaled, columns = X_test.columns, index = X_test.index)

print('After Scaler - Training set statistics')
print(X_train_scaled.describe())

"""Multiple Linear Regression"""

multiple_model = LinearRegression()

# training the model
multiple_model.fit(X_train_scaled, y_train)

# Make predictions
y_pred_train = multiple_model.predict(X_train_scaled)

y_pred_test = multiple_model.predict(X_test_scaled)

# calculate performance metrics for both training and testing sets
def calculate_metrics(y_true, y_pred, dataset):
  mse = mean_squared_error(y_true, y_pred)
  rmse = np.sqrt(mse)
  mae = mean_absolute_error(y_true, y_pred)
  r2 = r2_score(y_true, y_pred)

  print(f'Performance Metrics: {dataset}')
  print(f"Mean Squared Error (MSE): {mse:.4f}")
  print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
  print(f"Mean Absolute Error (MAE): {mae:.4f}")
  print(f"R-squared (R²) Score: {r2:.4f}")

# Training test performance
calculate_metrics(y_train, y_pred_train, 'Training')

# Testing Data Performance
calculate_metrics(y_test, y_pred_test, 'Testing')

"""Defining models and hyperparameter tuning

"""

models = {
    "Linear Regression": LinearRegression(),
    "Ridge Regression": Ridge(),
    "Lasso Regression": Lasso(),
    "Random Forest": RandomForestRegressor(random_state=42),
    "Gradient Boosting": GradientBoostingRegressor(random_state=42),
    "XGBoost": xgb.XGBRegressor(random_state=42, eval_metric='rmse', use_label_encoder=False),
}

# Example hyperparameter grids (optional)
param_grids = {
    "Ridge Regression": {'alpha': [0.1, 1.0, 10.0]},
    "Lasso Regression": {'alpha': [0.01, 0.1, 1.0]},
    "Random Forest": {'n_estimators': [50, 100], 'max_depth': [None, 10, 20]},
    "Gradient Boosting": {'n_estimators': [50, 100], 'learning_rate': [0.05, 0.1]},
    "XGBoost": {'n_estimators': [50, 100], 'learning_rate': [0.05, 0.1], 'max_depth': [3, 5]}
}

#Creating a function to train the model
def fit_model(name, model, X_train, y_train):
    if name in param_grids:
        grid = GridSearchCV(model, param_grids[name], cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1)
        grid.fit(X_train, y_train)
        print(f"[{name}] Best params: {grid.best_params_}")
        return grid.best_estimator_
    else:
        model.fit(X_train, y_train)
        return model

#Creating a function to evaluatae the model
def evaluate_model(name, model, X_test, y_test):
    y_pred = model.predict(X_test)
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)
    print(f"{name} Performance on Test Set:")
    print(f"  MAE: {mae:.4f}")
    print(f"  RMSE: {rmse:.4f}")
    print(f"  R²: {r2:.4f}")
    print(f"  Explained Variance: {r2*100:.2f}%")
    print()
    return {'Model': name, 'MAE': mae, 'RMSE': rmse, 'R2': r2}

# Initializing Results to hold prefmormance metrics.
trained_models = {}
results = []

for name, model in models.items():
    print(f"Training {name}...")
    best_model = fit_model(name, model, X_train, y_train)
    trained_models[name] = best_model                  # 👈 save it
    metrics = evaluate_model(name, best_model, X_test, y_test)
    results.append(metrics)

# results = []

# for name, model in models.items():
#     print(f"Training {name}...")
#     best_model = fit_model(name, model, X_train, y_train)
#     metrics = evaluate_model(name, best_model, X_test, y_test)
#     results.append(metrics)

#Results → DataFrame and rank
results_df = pd.DataFrame(results)
results_df = results_df.sort_values("R2", ascending=False).reset_index(drop=True)
print("=== Ranked (Test) ===")
print(results_df)

fig, axes = plt.subplots(2, 3, figsize=(18, 10))
fig.subplots_adjust(hspace=0.4, wspace=0.3)

# --- Row 1: Bar charts ---
# 1. R²
axes[0, 0].bar(top3["Model"], top3["R2"], color='skyblue')
axes[0, 0].set_title("Top 3 Models — R² (higher is better)")
axes[0, 0].set_ylabel("R²")
axes[0, 0].set_ylim(0, 1)
for i, v in enumerate(top3["R2"]):
    axes[0, 0].text(i, v+0.01, f"{v:.3f}", ha="center")

# 2. RMSE
axes[0, 1].bar(top3["Model"], top3["RMSE"], color='orange')
axes[0, 1].set_title("Top 3 Models — RMSE (lower is better)")
axes[0, 1].set_ylabel("RMSE")
for i, v in enumerate(top3["RMSE"]):
    axes[0, 1].text(i, v*1.01, f"{v:,.0f}", ha="center")

# 3. MAE
axes[0, 2].bar(top3["Model"], top3["MAE"], color='green')
axes[0, 2].set_title("Top 3 Models — MAE (lower is better)")
axes[0, 2].set_ylabel("MAE")
for i, v in enumerate(top3["MAE"]):
    axes[0, 2].text(i, v*1.01, f"{v:,.0f}", ha="center")

# --- Row 2: Predicted vs Actual & Residuals ---
best_name = top3.iloc[0]["Model"]
if 'trained_models' in globals() and best_name in trained_models:
    best_model = trained_models[best_name]
    y_pred_best = best_model.predict(X_test)

    # Scatter: Predicted vs Actual
    axes[1, 0].scatter(y_test, y_pred_best, alpha=0.4, s=8)
    z = np.polyfit(y_test, y_pred_best, 1)
    p = np.poly1d(z)
    xs = np.linspace(y_test.min(), y_test.max(), 100)
    axes[1, 0].plot(xs, p(xs), 'r--', alpha=0.8)
    axes[1, 0].set_title(f"Predicted vs Actual — {best_name}")
    axes[1, 0].set_xlabel("Actual Revenue")
    axes[1, 0].set_ylabel("Predicted Revenue")

    # Residuals histogram
    residuals = y_test - y_pred_best
    axes[1, 1].hist(residuals, bins=60, edgecolor='k', alpha=0.7)
    axes[1, 1].set_title(f"Residuals Distribution — {best_name}")
    axes[1, 1].set_xlabel("y_true - y_pred")
    axes[1, 1].set_ylabel("Frequency")

# Hide any unused subplot (last slot in row 2)
axes[1, 2].axis('off')

plt.show()

#######
ARTS = Path("artifacts")

st.subheader("📊 Dataset Overview")

# Small summary table
summary_p = ARTS / "data_summary.csv"
if summary_p.exists():
    st.write("**Data Summary**")
    st.dataframe(pd.read_csv(summary_p), use_container_width=True)
else:
    st.info("Run the EDA export cell to generate data_summary.csv")

# Missing values table
missing_p = ARTS / "missing_values.csv"
if missing_p.exists():
    with st.expander("Missing values by column"):
        st.dataframe(pd.read_csv(missing_p), use_container_width=True)

# Plots gallery (only show if they exist)
gallery = [
    ("Revenue Distribution", "revenue_distribution.png"),
    ("Top Countries by Revenue", "top_countries.png"),
    ("Top Products by Revenue", "top_products.png"),
    ("Revenue by Hour", "revenue_by_hour.png"),
    ("Revenue by Month", "revenue_by_month.png"),
]

cols = st.columns(2)
i = 0
for title, fname in gallery:
    p = ARTS / fname
    if p.exists():
        with cols[i % 2]:
            st.markdown(f"**{title}**")
            st.image(str(p), use_container_width=True)
        i += 1

if i == 0:
    st.info("No dataset plots found yet. Run the EDA export cell in your notebook to create them.")
